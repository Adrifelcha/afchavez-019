barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
sqrt(9)
sqrt(.35)
.35*.35
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.01,.29,.25,.1)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.25,.2,.15) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.15,.2,.25,.4)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.25,.2,.15) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.1,.1,.4,.4)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.4,.1,.1) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
scores
max(scores)
min(scores)
4*20
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.1,.1,.8,.6)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.4,.1,.1) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
scores
length(scores)
setwd("D:/afchavez/Desktop/Adrifelcha_PsicometriaYEvaluacion/Diagnostico_EB/R/TTVs")
dir()
rm(list=ls())
######
Dir_Pree <- "DIR-PREE (versión 1).csv"
Dir_Pree <- read.csv(Dir_Pree)
######
Dir_Prim <- "DIR-PRI (versión 1).csv"
Dir_Prim <- read.csv(Dir_Prim)
######
Dir_Sec <- "DIR-SEC (versión 1).csv"
Dir_Sec <- read.csv(Dir_Sec)
######
Dir_Esp <- "DIR-ESPE (Versión 1).csv"
Dir_Esp <- read.csv(Dir_Esp)
######
Sub_Pre <- "SUB-PRE.csv"
Sub_Pre <- read.csv(Sub_Pre)
######
Sub_Prim <- "SUB-PRIM.csv"
Sub_Prim <- read.csv(Sub_Prim)
######
Sub_Sec <- "SUB-SEC (versión 1).csv"
Sub_Sec <- read.csv(Sub_Sec)
######
Coord <- "COOR-SEC.csv"
Coord <- read.csv(Coord)
######
Sup_EF <- "SUPER-EDUFIS (versión 1).csv"
Sup_EF <- read.csv(Sup_EF)
######
Sup_Pre <- "SUPER-PRE (versión 1).csv"
Sup_Pre <- read.csv(Sup_Pre)
######
Sup_Prim <-"SUP-PRI (Versión 1).csv"
Sup_Prim <- read.csv(Sup_Prim)
######
Sup_Sec <-"SUPER-SEC (version_1).csv"
Sup_Sec <- read.csv(Sup_Sec)
######
Sup_Esp <- "SUPER-ESPE (versión 1).csv"
Sup_Esp <- read.csv(Sup_Esp)
######
Sup_Ad <- "SUPER-ADUL.csv"
Sup_Ad <- read.csv(Sup_Ad)
######
Jefe_Pre <- "JEFE-PRE.csv"
Jefe_Pre <- read.csv(Jefe_Pre)
######
Jefe_Prim <- "JEFE-PRIM (Versión 1).csv"
Jefe_Prim <- read.csv(Jefe_Prim)
######
Jefe_Tele <- "JEFE-TELE (Versión 1).csv"
Jefe_Tele <- read.csv(Jefe_Tele)
JefeS_Art <- "ENSE-SEC-ART.csv"
JefeS_Art <- read.csv(JefeS_Art)
####
JefeS_His <- "ENSE-SEC-HIS.csv"
JefeS_His <- read.csv(JefeS_His)
####
JefeS_Ing <- "ENSE-SEC-ING.csv"
JefeS_Ing <- read.csv(JefeS_Ing)
####
JefeS_Esp <- "ENSE-SEC-ESP.csv"
JefeS_Esp <- read.csv(JefeS_Esp)
####
JefeS_FCE <- "ENSE-SEC-FCE.csv"
JefeS_FCE <- read.csv(JefeS_FCE)
####
JefeS_Fis <- "ENSE-SEC-FIS.csv"
JefeS_Fis <- read.csv(JefeS_Fis)
####
JefeS_Geo <- "ENSE-SEC-GEO.csv"
JefeS_Geo <- read.csv(JefeS_Geo)
####
JefeS_Mate <- "ENSE-SEC-MAT.csv"
JefeS_Mate <- read.csv(JefeS_Mate)
####
JefeS_Quim <- "ENSE-SEC-QUI.csv"
JefeS_Quim <- read.csv(JefeS_Quim)
####
JefeS_Bio <- "ENSE-SEC-BIO.csv"
JefeS_Bio <- read.csv(JefeS_Bio)
####
JefeS_Tec <- "ENSE-SEC-TEC.csv"
JefeS_Tec <- read.csv(JefeS_Tec)
####
ATP_LEN_Pre <- "ATP-LEN-PRE (version_1).csv"
ATP_LEN_Pre <- read.csv(ATP_LEN_Pre)
####
ATP_LEN_Prim <- "ATP-LEN-PRI (versión 1).csv"
ATP_LEN_Prim <- read.csv(ATP_LEN_Prim)
####
ATP_LEN_Sec <- "ATP-LEN-SEC (versión 1).csv"
ATP_LEN_Sec <- read.csv(ATP_LEN_Sec)
####
ATP_PEN_Pre <- "ATP_PENS_PRE.csv"
ATP_PEN_Pre <- read.csv(ATP_PEN_Pre)
####
ATP_PEN_Prim <- "ATP-PEN-PRI (versión 1).csv"
ATP_PEN_Prim <- read.csv(ATP_PEN_Prim)
####
ATP_PEN_Sec <- "ATP_PENS_SEC.csv"
ATP_PEN_Sec <- read.csv(ATP_PEN_Sec)
####
ATP_Esp <- "ATP_ESPECIAL.csv"
ATP_Esp <- read.csv(ATP_Esp)
####
ATP_EF <- "ATP-EDUFIS (versión 1).csv"
ATP_EF <- read.csv(ATP_EF)
####
Dir <- "COMUN-DIR.csv"
Dir <- read.csv(Dir)
####
Sup <-"COMÚN-SUPER.csv"
Sup <- read.csv(Sup)
####
ATP <- "COMUN-ATP.csv"
ATP <- read.csv(ATP)
unique(ATP_PEN_Sec$Clave.reactivo)
length(unique(ATP_PEN_Sec$Clave.reactivo))-1
unique(ATP_EF$Clave.reactivo)
length(unique(ATP_EF$Clave.reactivo))
