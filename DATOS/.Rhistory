# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.15,.2,.25,.4)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.25,.2,.15) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.1,.1,.4,.4)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.4,.1,.1) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
scores
max(scores)
min(scores)
4*20
# Cronbach's alpha
# Code by José Luis Baroja
# Tuned and commented by Adriana
rm(list=ls())
###############################################
###############################################
# Simulate data
###############################################
set.seed(122)  #We fix our environment so it always generates the same random values
K <- 20          # items
n_obs <- 50      # participants
dta <- array(dim=c(n_obs,K))      #Empty array to hold all k's and n_obs
######## Fixing the probabilities of getting a specific response within a 4-level scale
for(ii in 1:K){                        # iterate to fill BY ITEM
if(ii %in% c(5,10,15)){                     # these are "strange" items
prob_vector <- c(.1,.1,.8,.6)           # with lower probabilities
prob_abnormal <- prob_vector
}else{
prob_vector <- c(.4,.4,.1,.1) # 4 levels per item      #probabilities for 'regular' items
prob_normal <- prob_vector
}
dta[,ii] <- sample(c(1:4),size=n_obs,replace = T,prob = prob_vector)
# Sample #n_obs responses within the range 1:4 with prob_vector probability per level, allowing replacement per extraction
}
scores <- apply(dta,MARGIN=1,FUN=sum)    #We obtain the total scores by summing each row
###############################################
# We plot the proportion of 1,2,3,4 per subject
###############################################
# 17 "high" items, 3 "low" items:
plot(NULL,xlim=c(1,4),ylim=c(0,1), ylab="Proportion", xlab="Response", axes=FALSE) #Empty plot
for(ii in 1:K){               #Insert a line per item
tb <- table(dta[,ii])       #Frequency of 1,2,3,4 observed (per item)
lines(as.numeric(names(tb)),tb/n_obs,       #Lines that show the relation between each step and the proportion at which it appears
lwd=1.5,col=c('red','blue')[(!ii%in%c(5,10,15))+1])
axis(1, c(1,2,3,4), c("1", "2", "3", "4"))
axis(2, c(0,0.25,0.5,0.75,1), c("0","0.25", "0.5", "0.75", "1"))
}
layout(matrix(1:2,ncol=2))
barplot(prob_normal, ylim=c(0,1), main="Usual probabilities", col="turquoise",
ylab="Probabilities", xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
barplot(prob_abnormal, ylim=c(0,1), main="Atypical probabilities", col="indianred",
xlab="Responses", cex.lab=1.5, font.lab=6)
axis(1, c(0.65,1.88,3.1,4.25), c("1", "2", "3", "4"))
############################################
# Cronbach's alpha as implemented in 'psych'
############################################
library('psych')             #Library for personality, psychometric theory and experimental psychology
# Cronbach's alpha from the package
alpha_psych <- alpha(dta)    #We run a Cronbach's alpha on our data set (all responses)
# Cronbach's alpha from scratch 1
var_x <- var(scores)   #Variance of the total scores
var_y <- apply(dta,MARGIN=2,FUN=var)  #Variance of the responses given by item
# MARGIN = 1 for rows ; 2 for columns ; c(1,2) for both
alpha_oldschl_1 <- (K/(K-1))*(1-(sum(var_y)/var_x))
# Cronbach's alpha from scratch 2
cor_mat <- cor(dta) # correlation matrix based on our data
cov_mat <- cov(dta) # covariance matrix based on our data
non_redundant_cor <- NULL   #Empty arrays to be filled with a for(){}
non_redundant_cov <- NULL
for(ci in 1:(nrow(cor_mat)-1)){       #For each row-1 in the cor_mat (a.k.a. Items-1)
#We will only take into consideration the Covariances and Correlations that are NOT redundant
#by taking just the values from one side of the diagonal
non_redundant_cor <- append(non_redundant_cor,       #We fill the non_redundant_cor object
cor_mat[ci,(ci+1):K])    #With each element on one side of the main diagonal
non_redundant_cov <- append(non_redundant_cov,
cov_mat[ci,(ci+1):K])
}
#We apply another 2 different formulas to get a Cronbach's alpha
alpha_oldschl_2 <- (K*mean(non_redundant_cov))/(mean(var_y)+(K-1)*mean(non_redundant_cov))
alpha_std_oldschl <- (K*mean(non_redundant_cor))/(1+(K-1)*mean(non_redundant_cor))
########################################################
# We print our results
########################################################
#We print all our computated alphas
print(paste("(R) Raw alpha: ", round(alpha_psych$total$raw_alpha,5))) # "Normal" (?) alpha
print(paste("(R) Standarized alpha; ", round(alpha_psych$total$std.alpha,5))) # Standarized alpha
print(paste("Variance-based formula: ", round(alpha_oldschl_1,5)))
print(paste("Covariance-based formula: ", round(alpha_oldschl_2,5)))
print(paste("Correlation-based formula; ", round(alpha_std_oldschl,5)))
##################################################################
#R automatically generated output
alpha_psych
dta
scores
length(scores)
setwd("D:/afchavez/Desktop/Adrifelcha_Work/1-AdaptacionEtapa1/DATOS")
dir()
rm(list=ls())
library("foreign")
###############
datos <- "unidad_de_calificacion_l.dbf"
data <- read.dbf(datos, as.is = TRUE)
##############
###############
###############
# Segmentamos los datos por figura
n_per_figure <- NULL   #Total de datos contenidos en cada Figura
for(figura in sort(unique(data$ID_EXAM9))){
n_per_figure[figura] <- sum(data$ID_EXAM9==figura)
}
n_per_figure
#Establecemos qué datos pertenecen a cada figura
figuras <- sort(unique(data$ID_EXAM9))
ATP <- c(1:sum(data$ID_EXAM9=='EDIRPDATP0218'))
Dir <- c((tail(ATP,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDIR0218')+tail(ATP,n=1)))
Doc0218 <- c((tail(Dir,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0218')+tail(Dir,n=1)))
Doc0318 <- c((tail(Doc0218,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0318')+tail(Doc0218,n=1)))
SAC <- c((tail(Doc0318,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSAC0218')+tail(Doc0318,n=1)))
SGE <- c((tail(SAC,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSGE0218')+tail(SAC,n=1)))
################
################
### Número de Respuestas por Cadena
r_9<- NULL
r_10<- NULL
for(i in 1:length(data$CURP)){
r_9[i] <- nchar(data$RESP_9[i])
r_10[i] <- nchar(data$RESP_10[i])
}
unique(r_9[ATP])
unique(r_9[Dir])
unique(r_9[Doc0218])
unique(r_9[Doc0318])
unique(r_9[SAC])
unique(r_9[SGE])
unique(r_10[ATP])
unique(r_10[Dir])
unique(r_10[Doc0218])
unique(r_10[Doc0318])
unique(r_10[SAC])
unique(r_10[SGE])
setwd("D:/afchavez/Desktop/Adrifelcha_Work/1-AdaptacionEtapa1/DATOS")
dir()
rm(list=ls())
library("foreign")
###############
datos <- "unidad_de_calificacion_l.dbf"
data <- read.dbf(datos, as.is = TRUE)
##############
###############
###############
# Segmentamos los datos por figura
n_per_figure <- NULL   #Total de datos contenidos en cada Figura
for(figura in sort(unique(data$ID_EXAM9))){
n_per_figure[figura] <- sum(data$ID_EXAM9==figura)
}
n_per_figure
#Establecemos qué datos pertenecen a cada figura
figuras <- sort(unique(data$ID_EXAM9))
ATP <- c(1:sum(data$ID_EXAM9=='EDIRPDATP0218'))
Dir <- c((tail(ATP,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDIR0218')+tail(ATP,n=1)))
Doc0218 <- c((tail(Dir,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0218')+tail(Dir,n=1)))
Doc0318 <- c((tail(Doc0218,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0318')+tail(Doc0218,n=1)))
SAC <- c((tail(Doc0318,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSAC0218')+tail(Doc0318,n=1)))
SGE <- c((tail(SAC,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSGE0218')+tail(SAC,n=1)))
################
################
### Número de Respuestas por Cadena
r_9<- NULL
r_10<- NULL
for(i in 1:length(data$CURP)){
r_9[i] <- nchar(data$RESP_9[i])
r_10[i] <- nchar(data$RESP_10[i])
}
sort(unique(r_9[ATP]))
sort(unique(r_9[Dir]))
sort(unique(r_9[Doc0218]))
sort(unique(r_9[Doc0318]))
sort(unique(r_9[SAC]))
sort(unique(r_9[SGE]))
sort(unique(r_10[ATP]))
sort(unique(r_10[Dir]))
sort(unique(r_10[Doc0218]))
sort(unique(r_10[Doc0318]))
sort(unique(r_10[SAC]))
sort(unique(r_10[SGE]))
setwd("D:/afchavez/Desktop/Adrifelcha_Work/1-AdaptacionEtapa1/DATOS")
dir()
rm(list=ls())
library("foreign")
###############
datos <- "unidad_de_calificacion_l.dbf"
data <- read.dbf(datos, as.is = TRUE)
##############
###############
###############
# Segmentamos los datos por figura
n_per_figure <- NULL   #Total de datos contenidos en cada Figura
for(figura in sort(unique(data$ID_EXAM9))){
n_per_figure[figura] <- sum(data$ID_EXAM9==figura)
}
n_per_figure
#Establecemos qué datos pertenecen a cada figura
figuras <- sort(unique(data$ID_EXAM9))
ATP <- c(1:sum(data$ID_EXAM9=='EDIRPDATP0218'))
Dir <- c((tail(ATP,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDIR0218')+tail(ATP,n=1)))
Doc0218 <- c((tail(Dir,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0218')+tail(Dir,n=1)))
Doc0318 <- c((tail(Doc0218,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0318')+tail(Doc0218,n=1)))
SAC <- c((tail(Doc0318,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSAC0218')+tail(Doc0318,n=1)))
SGE <- c((tail(SAC,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSGE0218')+tail(SAC,n=1)))
################
################
### Número de Respuestas por Cadena
r_9<- NULL
r_10<- NULL
for(i in 1:length(data$CURP)){
r_9[i] <- nchar(data$RESP_9[i])
r_10[i] <- nchar(data$RESP_10[i])
}
sort(unique(r_9[ATP]))
unique(r_9[ATP])
sort(unique(r_9[Dir]))
sort(unique(r_9[Doc0218]))
sort(unique(r_9[Doc0318]))
sort(unique(r_9[SAC]))
sort(unique(r_9[SGE]))
sort(unique(r_10[ATP]))
sort(unique(r_10[Dir]))
sort(unique(r_10[Doc0218]))
sort(unique(r_10[Doc0318]))
sort(unique(r_10[SAC]))
sort(unique(r_10[SGE]))
setwd("D:/afchavez/Desktop/Adrifelcha_Work/1-AdaptacionEtapa1/DATOS")
dir()
rm(list=ls())
library("foreign")
###############
datos <- "unidad_de_calificacion_l.dbf"
data <- read.dbf(datos, as.is = TRUE)
##############
###############
###############
# Segmentamos los datos por figura
n_per_figure <- NULL   #Total de datos contenidos en cada Figura
for(figura in sort(unique(data$ID_EXAM9))){
n_per_figure[figura] <- sum(data$ID_EXAM9==figura)
}
n_per_figure
#Establecemos qué datos pertenecen a cada figura
figuras <- sort(unique(data$ID_EXAM9))
ATP <- c(1:sum(data$ID_EXAM9=='EDIRPDATP0218'))
Dir <- c((tail(ATP,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDIR0218')+tail(ATP,n=1)))
Doc0218 <- c((tail(Dir,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0218')+tail(Dir,n=1)))
Doc0318 <- c((tail(Doc0218,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDDOC0318')+tail(Doc0218,n=1)))
SAC <- c((tail(Doc0318,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSAC0218')+tail(Doc0318,n=1)))
SGE <- c((tail(SAC,n=1)+1) : (sum(data$ID_EXAM9=='EDIRPDSGE0218')+tail(SAC,n=1)))
################
################
### Número de Respuestas por Cadena
r_9<- NULL
r_10<- NULL
for(i in 1:length(data$CURP)){
r_9[i] <- nchar(data$RESP_9[i])
r_10[i] <- nchar(data$RESP_10[i])
}
sort(unique(r_9[ATP]))
sort(unique(r_9[Dir]))
sort(unique(r_9[Doc0218]))
sort(unique(r_9[Doc0318]))
sort(unique(r_9[SAC]))
sort(unique(r_9[SGE]))
sort(unique(r_10[ATP]))
sort(unique(r_10[Dir]))
sort(unique(r_10[Doc0218]))
sort(unique(r_10[Doc0318]))
sort(unique(r_10[SAC]))
sort(unique(r_10[SGE]))
?as.character
as.character(ATP)
as.character(data$RESP_9[1])
(data$RESP_9[1])
strsplit(data$ID_EXAM9[1])
strsplit(data$ID_EXAM9[1], fixed=TRUE)
strsplit(data$ID_EXAM9[1], split=1)
strsplit(data$RESP_9[1], split=1)
strsplit(data$RESP_9, split=1)
strsplit(data$RESP_9[1], split=1)
strsplit(data$RESP_9[1], split=4)
sqrt(152.031)
